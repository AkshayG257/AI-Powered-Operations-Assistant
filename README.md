# OPS-AI-ASSISTANT

An **AI-assisted operational analytics tool** that converts raw metrics into clear, human-readable insights using a **pluggable AI backend**.

This project demonstrates how traditional data processing can be enhanced with **local, openâ€‘source AI models** (via Ollama) while keeping the architecture flexible enough to swap AI providers later.

---

### ğŸ” Sample AI-Generated Insight (API Response)

Below is an example response generated by the system after analyzing an uploaded CSV file.  
The service dynamically validates input, computes metrics, and generates contextual insights using a local LLM.

![AI Insight Output](./assets/ai-insight-response.png)

## ğŸš€ What this project does
- Loads and validates operational / business metrics
- Computes useful metrics programmatically
- Uses an AI model to generate **executiveâ€‘style insight summaries**
- Works **without OpenAI or paid APIs**
- Designed to run **locally first**, deployable later

Example output includes:
- Key observations
- Anomalies
- Risks and improvement areas

---

## ğŸ§  Why this project exists
Instead of *chasing companies* with resumes alone, this project focuses on **building capability**:

> *A system that blends analytics, backend engineering, and applied AI.*

It is intentionally simple, readable, and practical â€” mirroring real internal tools used in startups and operations teams.

---

## ğŸ—ï¸ Architecture (current)
```
app/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ai_client.py      # AI backend (Ollama today, swappable later)
â”‚   â”œâ”€â”€ ai_summary.py     # AI-driven insight generation
â”‚   â”œâ”€â”€ metrics.py        # Metric calculations
â”‚   â””â”€â”€ validator.py      # Input validation
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ data_loader.py    # CSV / data loading
â”‚
â”œâ”€â”€ main.py               # Entry point
â””â”€â”€ config.py             # Feature flags
```

AI is intentionally isolated behind a service layer so it can be replaced with:
- Hosted openâ€‘source APIs
- Cloud inference services
- Mock implementations

---

## ğŸ¤– AI Backend (current)
- **Ollama** (local inference server)
- Model: `mistral`
- No API keys
- No quotas
- Fully offline capable

---

## â–¶ï¸ How to run locally

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Install & start Ollama
```bash
ollama serve
ollama pull mistral
```

### 3. Run the app
```bash
python -m app.main
```

Or test AI directly:
```bash
python -m app.services.ai_summary
```

---

## ğŸ§ª Current status
- âœ… Data loading
- âœ… Metric computation
- âœ… Local AI integration
- âœ… Insight generation

---

## ğŸ”® Planned next steps
- Optional deployment on EC2
- Swap AI backend to a free hosted openâ€‘source API for public demo
- Improve prompt structure
- Add structured (JSON) AI output

---

## ğŸ“Œ Key takeaway
This is not a demo toy.

It is a **realistic, extensible foundation** for AIâ€‘assisted analytics â€” built to learn, iterate, and showcase applied engineering skills.

---

*Built with focus, clarity, and curiosity.*

